import mlflow 

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("nyc-taxi-experiment")

if 'data_exporter' not in globals():
    from mage_ai.data_preparation.decorators import data_exporter


@data_exporter
def export_data(data, *args, **kwargs):
    """
    Exports data to some source.

    Args:
        data: The output from the upstream parent block
        args: The output from any additional upstream blocks (if applicable)

    Output (optional):
        Optionally return any object and it'll be logged and
        displayed when inspecting the block run.
    """

    dv, lr = data
    
    with mlflow.start_run():
        mlflow.set_tag("developer", "Jack")

        # Log the linear regression model
        mlflow.sklearn.log_model(lr, "linear_regression_model")

        # Serialize and save dv as a file
        with open("dict_vectorizer.pkl", "wb") as f:
            pickle.dump(dv, f)

        # Log the serialized dv file as an artifact
        mlflow.log_artifact("dict_vectorizer.pkl")
    
    return "Data exported successfully"
